# Character-Level RNN for Text Generation

A PyTorch implementation of a multi-layer Recurrent Neural Network (RNN) for character-level text generation

## Overview

This project implements a character-level language model using vanilla RNN architecture. The model is trained on a transformer-related text corpus and learns to generate similar technical text character by character.

## Advantages of RNNs

1. **Sequential Data Processing**: RNNs naturally handle sequential data by maintaining an internal hidden state that captures information from previous timesteps.

2. **Variable Length Input**: Unlike traditional neural networks, RNNs can process inputs of varying lengths, making them ideal for text, speech, and time series data.

3. **Parameter Sharing**: The same weights are used across all timesteps, resulting in fewer parameters compared to fully connected networks for sequential data.

4. **Memory Mechanism**: The hidden state acts as a memory that can theoretically capture dependencies from earlier in the sequence.

## ‚ö†Ô∏è Disadvantages of RNNs

1. **Vanishing Gradient Problem**: During backpropagation through time, gradients can exponentially decay, making it difficult to learn long-range dependencies (typically limited to ~10-20 timesteps effectively).

2. **Exploding Gradient Problem**: Conversely, gradients can also explode, requiring gradient clipping to stabilize training.

3. **Sequential Computation**: RNNs process sequences step-by-step, which cannot be parallelized, leading to slower training and inference compared to transformers.

4. **Limited Context Window**: Vanilla RNNs struggle to remember information from many timesteps ago, even with techniques like gradient clipping.

5. **Training Difficulty**: RNNs are notoriously difficult to train, requiring careful tuning of learning rates, initialization, and architecture choices.

## üèóÔ∏è Implementation Details

### Architecture

The implementation consists of three main components:

1. **RNNCell**: A single RNN cell implementing the core recurrent computation:
   ```
   h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)
   y_t = W_hy * h_t
   ```

2. **RNN**: A multi-layer RNN that stacks multiple RNN cells with dropout regularization between layers.

3. **CharRNN**: The complete character-level model with a final linear layer to project hidden states to vocabulary size.

### Model Configuration

```python
vocab_size = 25 (chars in toy dateset)     # Unique characters in dataset
hidden_size = 256    # Hidden state dimension
no_of_layers = 3     # Number of stacked RNN layers
dropout = 0.1        # Dropout probability between layers
context_len = 100    # Sequence length for training
batch_size = 64      # Training batch size
```

### Dataset

**Source**: `Datasets/transformer.txt` - A toy dataset generated by ChatGPT containing technical descriptions about transformer architecture.

**Dataset Statistics**:
- Total characters: ~300,000 characters ( generated by chat-gpt)
- Content: Technical text about transformers, attention mechanisms, encoders, decoders, etc.

## üéØ Demo Output

```
a attention allows the model to capture different types of relationships 
transformers replace recurrent networks with self attention layers scaled dot 
product attention computes attention scores efficiently the decoder generates 
the output sequence autoregressively residual connections help gradients flow 
in deep networks the encoder reads the input sequence and produces hidden 
representations layer normalization stabilizes training of transformers position 
encoding provides information about token order transformers replace recurrent 
networks with self attention layers multi head attention allows the model to 
capture different types of relationships position encoding provides information 
about token order residual connections help gradients flow in deep networks 
transformers replace recurrent networks with self attention layers transformers 
replace recurrent networks with self attention layers attention allows the model 
to focus on important parts of the input feed forward networks transform the 
representation between attention layers feed forward networks transform the 
representation between attention layers the decoder generates the output sequence 
autoregressively scaled dot product attention computes attention scores efficiently 
transformers replace recurrent networks
```

## üìö References

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [PyTorch RNN](https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html)


