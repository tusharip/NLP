# Character-Level RNN for Text Generation

A PyTorch implementation of a multi-layer Recurrent Neural Network (RNN) for character-level text generation

## Overview

This project implements a character-level language model using vanilla RNN architecture. The model is trained on a transformer-related text corpus and learns to generate similar technical text character by character.

## Advantages of RNNs

1. **Sequential Data Processing**: RNNs naturally handle sequential data by maintaining an internal hidden state that captures information from previous timesteps.

2. **Variable Length Input**: Unlike traditional neural networks, RNNs can process inputs of varying lengths, making them ideal for text, speech, and time series data.

3. **Parameter Sharing**: The same weights are used across all timesteps, resulting in fewer parameters compared to fully connected networks for sequential data.

4. **Memory Mechanism**: The hidden state acts as a memory that can theoretically capture dependencies from earlier in the sequence.

## ‚ö†Ô∏è Disadvantages of RNNs

1. **Vanishing Gradient Problem**: During backpropagation through time, gradients can exponentially decay, making it difficult to learn long-range dependencies (typically limited to ~10-20 timesteps effectively).

2. **Exploding Gradient Problem**: Conversely, gradients can also explode, requiring gradient clipping to stabilize training.

3. **Sequential Computation**: RNNs process sequences step-by-step, which cannot be parallelized, leading to slower training and inference compared to transformers.

4. **Limited Context Window**: Vanilla RNNs struggle to remember information from many timesteps ago, even with techniques like gradient clipping.

5. **Training Difficulty**: RNNs are notoriously difficult to train, requiring careful tuning of learning rates, initialization, and architecture choices.

## üèóÔ∏è Implementation Details

### Architecture

The implementation consists of three main components:

1. **RNNCell**: A single RNN cell implementing the core recurrent computation:
   ```
   h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)
   y_t = W_hy * h_t
   ```

2. **RNN**: A multi-layer RNN that stacks multiple RNN cells with dropout regularization between layers.

3. **CharRNN**: The complete character-level model with a final linear layer to project hidden states to vocabulary size.

### Model Configuration

```python
vocab_size = 25 (chars in toy dateset)     # Unique characters in dataset
hidden_size = 256    # Hidden state dimension
no_of_layers = 3     # Number of stacked RNN layers
dropout = 0.1        # Dropout probability between layers
context_len = 100    # Sequence length for training
batch_size = 64      # Training batch size
```

### Dataset

**Source**: `Datasets/transformer.txt` - A toy dataset generated by ChatGPT containing technical descriptions about transformer architecture.

**Dataset Statistics**:
- Total characters: ~300,000 characters ( generated by chat-gpt)
- Content: Technical text about transformers, attention mechanisms, encoders, decoders, etc.

## üéØ Demo Output

```
SEBASTIAN:
Not it
Would I be mightnister, go itselves I should she, and shall sit hear doth speak
Them?

SEBASTIAN:
It dost them they think should shore: sot shall such ate thy fiut them, sir, it:
What dost your sons.

ANTONIO:
Nex again and thing they sleep in my affeclan to seen in they fealts at the quitting.

ALONSO:
Whear-daugh;--strike o' the duke: betterstintty you as yetermeds so. Why, sir, foul son me this arms thy formwectorby a drumpets then speak thou stoop you the subjectne. The armiom of Mestant as not ninetant he loudy that suldnesseliak.

ADRIAN:
Not didst sholl sleeps
Didst should not that blother.

SEBASTIAN:
Lord, I should swore they not sweet did her laughark asly straight; I that least,
Not should, thee.

SEBASTIAN:
What lost at the tribe thee there speak; thy vantly all as a kess at thou dug they stranger
Spilents your trift fresh gonits the wory sure speak the spiss at your senian, they then the quelo, I do: they but discalous here of hath idmore. Why, by your seed, they your Does and
Them and she done. What speak miethy to a disperlod asky, and of a scrangey are the mouth'd I shall.
Thenctions
My brighty thank they the ssactise them, this some speak I am sight so sension thy brother should.

ANTONIO:
No, sir, Arm you findy talkin Dety
We satised sir, the lord, art
Deterness, that the gallentister
With son: it strangeney doth deself, I set it my signious first speak me: send are heavy
Thee fear you?

ANTONIO:
Nor nevereth meetless. The spoke so, and thing the block, compless sallaster they spetch'd he drease upode yet and speak as all beur slyselve that I speak on't!

ANTONIO:
None the sumper, free---Widrit
Should I your farther ashatter this sacry thou art as do it.
```

## üìö References

- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [PyTorch RNN](https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html)


